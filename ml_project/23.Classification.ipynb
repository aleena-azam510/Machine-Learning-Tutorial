{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638fd83d-884f-4365-920c-7ac3b314c583",
   "metadata": {},
   "source": [
    "Classification\n",
    "\n",
    "The Classification algorithm is used to identify the category of new observations on the basis of training data.\n",
    "\n",
    "In Classification, a program learns from the given dataset or observations and then classifies new observation into a number of classes or groups.   \n",
    "\n",
    "Such as, Yes or No, 0 or 1, Spam or Not Spam, cat or dog, etc. Classes can be called as targets/labels or categories.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2420ba1-2014-4fa9-84ce-8519fbbbee09",
   "metadata": {},
   "source": [
    "There are two types of Classifications:\n",
    "\n",
    "Binary Classifier: If the classification problem has only two possible outcomes, then it is called as Binary Classifier.\n",
    "\n",
    "Examples: SPAM or NOT SPAM, CAT or DOG, etc.\n",
    "Multi-class Classifier: If a classification problem has more than two outcomes, then it is called as Multi-class Classifier.\n",
    "\n",
    "Example: Classifications of types of crops, Classification of types of music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da7565-42fd-4855-a648-c109df2c35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES OF ML\n",
    "\n",
    "CLASSIFICATION ALGORITHMS\n",
    "\n",
    "Non-linear Models\n",
    "\n",
    "K-Nearest Neighbours\n",
    "SVM\n",
    "Naïve Bayes\n",
    "Decision Tree Classification\n",
    "Random Forest Classification\n",
    "\n",
    "Linear Models\n",
    "\n",
    "Logistic Regression\n",
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef26179-c345-42b5-bae1-fb63a484f1c9",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION\n",
    "\n",
    "Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique.\n",
    "\n",
    "It is used for predicting the categorical dependent variable using a given set of independent variables.   \n",
    "\n",
    "Therefore, the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.\n",
    "\n",
    "\n",
    "Types of Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical method used for predicting the probability of an event occurring. It's particularly useful when the dependent variable is categorical, meaning it can take on a limited number of values.\n",
    "\n",
    "The image categorizes Logistic Regression into three main types based on the nature of the dependent variable:\n",
    "\n",
    "Binomial Logistic Regression:\n",
    "\n",
    "This is the most common type.\n",
    "The dependent variable has only two possible outcomes (binary).\n",
    "Examples:\n",
    "Pass/Fail\n",
    "Yes/No\n",
    "0/1\n",
    "Multinomial Logistic Regression:\n",
    "\n",
    "The dependent variable can have three or more unordered categories.\n",
    "Examples:\n",
    "Types of animals (cat, dog, sheep)\n",
    "Colors (red, green, blue)\n",
    "Ordinal Logistic Regression:\n",
    "\n",
    "The dependent variable has three or more ordered categories.\n",
    "The order of the categories is meaningful.\n",
    "Examples:\n",
    "Low, Medium, High\n",
    "Strongly disagree, Disagree, Neutral, Agree, Strongly agree\n",
    "In essence:\n",
    "\n",
    "Binomial: Two choices\n",
    "Multinomial: Multiple unordered choices\n",
    "Ordinal: Multiple ordered choices\n",
    "\n",
    "\n",
    "\n",
    "Equation\n",
    "\n",
    "The core equation for Logistic Regression is:\n",
    "\n",
    "y = 1 / (1 + e^(-x))\n",
    "y: This represents the probability of the event occurring (dependent variable). In the example given, it's the probability of someone buying a product.\n",
    "x: This represents the input variable (independent variable). In the example, it's the person's salary.\n",
    "e: This is Euler's constant, approximately equal to 2.71828.\n",
    "How it Works\n",
    "\n",
    "Linear Regression Foundation: The Logistic Regression equation is derived from the Linear Regression equation. However, unlike Linear Regression, which predicts a continuous value, Logistic Regression predicts a probability.\n",
    "\n",
    "Sigmoid Function: The equation uses the sigmoid function (also known as the logistic function), which maps any real value to a value between 0 and 1. This makes it suitable for predicting probabilities.\n",
    "\n",
    "Probability Prediction: By plugging the input value (x) into the equation, you get a value between 0 and 1. This value represents the predicted probability of the event occurring.\n",
    "\n",
    "Example\n",
    "\n",
    "Let's say the equation is:\n",
    "\n",
    "y = 1 / (1 + e^(-0.01 * salary))\n",
    "If someone's salary is 50,000:\n",
    "\n",
    "y = 1 / (1 + e^(-0.01 * 50,000))\n",
    "Calculating this gives you a probability value between 0 and 1, representing the likelihood of that person buying the product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fffa6-7711-4919-81b0-27d20ba8628e",
   "metadata": {},
   "source": [
    "workflow of logistic\n",
    "data > input > linear regression>output>sigmoid function>true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b60aa0-21a5-457b-9fd0-83370826f8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30004595-276a-4992-8898-a3611f5f8ba2",
   "metadata": {},
   "source": [
    "A confusion matrix is like a report card for this model.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "The Model Predicts: The model looks at each email and guesses if it's spam or not.\n",
    "Reality Check: We compare the model's guesses to the actual truth (whether the email really was spam or not).\n",
    "The Matrix: The confusion matrix shows us how often the model got things right and wrong.\n",
    "Here's a simple example:\n",
    "\n",
    "Prediction\tActual Spam\tActual Not Spam\n",
    "Spam\t    80\t10\n",
    "Not Spam\t5\t95\n",
    "\n",
    "Export to Sheets\n",
    "Top Left (80): The model correctly identified 80 spam emails as spam (True Positives).\n",
    "Top Right (10): The model incorrectly flagged 10 non-spam emails as spam (False Positives).\n",
    "Bottom Left (5): The model missed 5 spam emails (False Negatives).\n",
    "Bottom Right (95): The model correctly identified 95 non-spam emails as non-spam (True Negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a1b6ef-7311-4f7c-b093-a1715e5a7b7d",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "Accuracy measures how often a model is correct overall. It's the proportion of correct predictions (true positives and true negatives) out of all predictions made.\n",
    "\n",
    "Precision\n",
    "Precision measures how often a model is correct when it predicts \"yes\" or \"positive\". It's the proportion of true positives out of all positive predictions.\n",
    "\n",
    "Recall\n",
    "Recall measures how often a model detects all the actual \"yes\" or \"positive\" cases. It's the proportion of true positives out of all actual positive cases.\n",
    "\n",
    "F1 Score\n",
    "The F1 score is the average of precision and recall. It balances the two and provides a single metric to evaluate the model's performance.\n",
    "\n",
    "Confusion Matrix\n",
    "A confusion matrix is a table that helps evaluate a model's performance. It has four main parts:\n",
    "True Positives (TP): Correctly predicted \"yes\" or \"positive\" cases.\n",
    "True Negatives (TN): Correctly predicted \"no\" or \"negative\" cases.\n",
    "False Positives (FP): Incorrectly predicted \"yes\" or \"positive\" cases.\n",
    "False Negatives (FN): Incorrectly predicted \"no\" or \"negative\" cases.\n",
    "\n",
    "Here's an example of a confusion matrix:\n",
    "Predicted\tActual Positive\tActual Negative\n",
    "Positive\tTP (100)\tFP (20)\n",
    "Negative\tFN (15)\t    TN (150)\n",
    "\n",
    "In this example:\n",
    "Accuracy = (TP + TN) / Total = (100 + 150) / 285 ≈ 0.87\n",
    "Precision = TP / (TP + FP) = 100 / (100 + 20) ≈ 0.83\n",
    "Recall = TP / (TP + FN) = 100 / (100 + 15) ≈ 0.87\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall) ≈ 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a359ed-4807-4472-8c12-8f5eb588a049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d88f38e-63c7-4909-96f6-eee836bf34b2",
   "metadata": {},
   "source": [
    "Techniques to Handle Imbalanced Datasets\n",
    "1. Random Over Sampling (ROS)\n",
    "How it works: Randomly duplicate samples from the minority class to increase its size.\n",
    "Pros: Simple to implement, increases the size of the minority class.\n",
    "Cons: May lead to overfitting, as the model sees duplicate samples.\n",
    "2. Random Under Sampling (RUS)\n",
    "How it works: Randomly remove samples from the majority class to decrease its size.\n",
    "Pros: Reduces the size of the dataset, may improve model training time.\n",
    "Cons: May discard important information, can lead to loss of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a238081-39ff-4754-9e69-2e5e179610e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c47bd1c-0286-4969-beb0-f83e5ee49cae",
   "metadata": {},
   "source": [
    "Naive Bayes is a machine learning algorithm that helps predict the likelihood of an event occurring based on prior knowledge of conditions that might be related to the event.\n",
    "How Does it Work?\n",
    "Bayes' Theorem: Naive Bayes is based on Bayes' theorem, which describes the probability of an event occurring given some prior knowledge of conditions.\n",
    "Prior Probability: The algorithm starts with a prior probability of each class (e.g., spam or not spam).\n",
    "Likelihood: It then calculates the likelihood of each feature (e.g., words in an email) given each class.\n",
    "Posterior Probability: The algorithm multiplies the prior probability by the likelihood to get the posterior probability of each class given the features.\n",
    "Prediction: The class with the highest posterior probability is chosen as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09a800-8eed-4cf4-b945-3f47cae0f754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "307bf68d-f101-4d64-9f47-f6b431f07dc0",
   "metadata": {},
   "source": [
    "Decision Tree\n",
    "\n",
    "Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems.\n",
    "\n",
    "In 1  order to build a tree, we use the CART algorithm, which stands for Classification and Regression Tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81e7c6-c65d-4cea-921f-1573517208cf",
   "metadata": {},
   "source": [
    "Decision trees have three main parts:\n",
    "\n",
    "1. Root Node:\n",
    "\n",
    "The starting point of the tree.\n",
    "Represents the entire dataset or population.\n",
    "The first decision or split is made at this node.\n",
    "2. Internal Nodes:\n",
    "\n",
    "Nodes that result from splitting the root node.\n",
    "Represent intermediate decisions or conditions within the tree.\n",
    "Each internal node has one or more branches leading to other nodes (either internal or leaf nodes).\n",
    "3. Leaf Nodes (or Terminal Nodes):\n",
    "\n",
    "The end points of the tree.\n",
    "Represent the final classification or outcome.\n",
    "No further splitting occurs at these nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089db54e-a89e-40e3-9108-62f603c96b38",
   "metadata": {},
   "source": [
    "1. Start Node (Root Node):\n",
    "\n",
    "The topmost node in the tree.\n",
    "Represents the entire dataset or population being analyzed.\n",
    "The initial point where the decision-making process begins.\n",
    "2. Splitting:\n",
    "\n",
    "The process of dividing a node into two or more sub-nodes based on a specific condition or attribute.\n",
    "The choice of attribute for splitting is crucial and is determined by algorithms like ID3, C4.5, or CART.\n",
    "The goal is to create sub-nodes that are more homogeneous (contain data points with similar characteristics) than the parent node.\n",
    "3. Decision Node:\n",
    "\n",
    "Any node that is further divided into sub-nodes through splitting.\n",
    "Represents an intermediate decision point within the tree.\n",
    "4. Terminal Node (Leaf Node):\n",
    "\n",
    "The end points of the tree.\n",
    "Represent the final classification or outcome of the decision-making process.\n",
    "No further splitting occurs at these nodes.\n",
    "5. Pruning:\n",
    "\n",
    "A technique used to simplify the decision tree by removing branches or sub-trees.\n",
    "Helps to prevent overfitting, which occurs when the tree is too complex and performs poorly on new, unseen data.\n",
    "Pruning can be done in two ways:\n",
    "Pre-pruning: Stopping the tree's growth early during the construction process.\n",
    "Post-pruning: Removing branches from a fully grown tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e2082-de72-4006-9ead-315ba5b9d212",
   "metadata": {},
   "source": [
    "Attribute Selection Methods in Decision Trees\n",
    "\n",
    "Attribute selection methods are crucial in decision tree algorithms as they determine which attribute should be used to split a node at each level. The goal is to select the attribute that provides the most information about the class labels, leading to purer sub-nodes.   \n",
    "\n",
    "Key Attribute Selection Measures:\n",
    "\n",
    "Information Gain:\n",
    "\n",
    "Measures the reduction in entropy (impurity) after splitting a dataset based on an attribute.\n",
    "Higher information gain indicates that the attribute is more effective in separating the data into distinct classes.   \n",
    "Calculated as:\n",
    "Information Gain(S, A) = Entropy(S) - Sum( |Sv|/|S| * Entropy(Sv) )\n",
    "where:\n",
    "S is the dataset\n",
    "A is the attribute\n",
    "Sv is the subset of S for which attribute A has value v\n",
    "Entropy:\n",
    "\n",
    "Measures the impurity or disorder of a dataset.   \n",
    "A higher entropy value indicates greater impurity (more mixed classes).\n",
    "Calculated as:\n",
    "Entropy(S) = - Sum( P(ci) * log2(P(ci)) )\n",
    "where:\n",
    "P(ci) is the probability of class ci in dataset S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3364cbc-64c2-4fac-9eda-cd23602d17cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53404d00-c90b-4fab-ae5b-f35e6a29dd28",
   "metadata": {},
   "source": [
    "KNN (k-Nearest Neighbors) in Simple Words\n",
    "\n",
    "Imagine you're trying to decide what type of music to listen to. You ask your friends for recommendations. In KNN, \"k\" represents the number of friends you ask. Let's say k=3.\n",
    "\n",
    "You ask your 3 closest friends (your \"nearest neighbors\") for their favorite music genres. Based on their responses, you decide what music to listen to. If 2 out of 3 friends like rock, you're more likely to choose rock music.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "Data Points: Each of your friends is a data point, with their music preferences being their \"features.\"\n",
    "New Data Point: You are the new data point, trying to determine your music preference.\n",
    "Distance: The \"distance\" between you and your friends is based on how similar your music tastes are.\n",
    "K-Nearest Neighbors: You find the k closest friends (in this case, 3).\n",
    "Classification: You classify yourself based on the majority vote of your k-nearest neighbors.\n",
    "In Machine Learning:\n",
    "\n",
    "KNN is used to classify new data points by looking at the classes of the k-nearest data points in the training dataset. It's a simple but effective algorithm for tasks like image recognition, recommendation systems, and anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c122cc1-4880-4ab1-908b-7e500a5a4863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70a4e49e-e0df-4307-9033-6e3f9b61d02a",
   "metadata": {},
   "source": [
    "SVM (Support Vector Machine) in Simple Words\n",
    "\n",
    "Imagine you have two types of fruits: apples and oranges. You want to create a rule to easily tell them apart. SVM helps you find the best \"line\" or \"boundary\" to separate these fruits.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "Data Points: Each fruit is a data point with features like size, color, and shape.\n",
    "Finding the Best Boundary: SVM aims to find the boundary (called a \"hyperplane\") that maximizes the distance between the closest points of each fruit type. These closest points are called \"support vectors.\"\n",
    "Classification: When you encounter a new fruit, you simply check on which side of the boundary it falls. If it's on the apple side, it's likely an apple.\n",
    "In Machine Learning:\n",
    "\n",
    "SVM is used for classification tasks. It's particularly good at finding clear boundaries between different classes, even in complex datasets.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Effective for High-Dimensional Data: SVM can handle data with many features.\n",
    "Versatile: Can be used for both linear and non-linear classification.\n",
    "Robust to Overfitting: SVM is less prone to overfitting, which means it generalizes well to new data.\n",
    "Example:\n",
    "\n",
    "Imagine you're classifying emails as spam or not spam. SVM can find a boundary that separates spam emails from non-spam emails based on features like word frequency, sender address, and subject line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac3671-607b-4d0c-97c0-d2b1b8b0f573",
   "metadata": {},
   "source": [
    "Hard Margin SVM\n",
    "\n",
    "Goal: To find a hyperplane that perfectly separates the data into two classes without any misclassifications.\n",
    "Ideal Scenario: When the data is linearly separable and there are no outliers.\n",
    "Challenge: In real-world scenarios, data is often not perfectly separable. Hard Margin SVM can be sensitive to outliers and may not generalize well to unseen data.\n",
    "Soft Margin SVM\n",
    "\n",
    "Goal: To find a hyperplane that allows for some misclassifications to achieve better generalization and robustness to outliers.\n",
    "Approach: Introduces a penalty for misclassifications, allowing the algorithm to find a balance between maximizing the margin and minimizing the number of errors.\n",
    "Flexibility: More suitable for real-world datasets with noise and outliers.\n",
    "Visualizing the Difference\n",
    "\n",
    "The image you provided illustrates the difference between Hard Margin and Soft Margin SVM:\n",
    "\n",
    "Hard Margin: The hyperplane perfectly separates the data, but it is very close to some data points, making it susceptible to outliers.\n",
    "Soft Margin: The hyperplane allows for a few misclassifications (data points on the wrong side), but it creates a wider margin, making it more robust to noise and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd4be0-7d7c-42bb-8303-3c14086f370f",
   "metadata": {},
   "source": [
    "The text describes two types of Support Vector Machines (SVMs):\n",
    "\n",
    "1. Simple SVM:\n",
    "\n",
    "Purpose: Typically used for linear regression and classification problems.\n",
    "Linearly Separable Data: Assumes the data can be separated by a straight line (or hyperplane in higher dimensions).\n",
    "2. Kernel SVM:\n",
    "\n",
    "Purpose: Handles non-linearly separable data.\n",
    "Flexibility: Introduces the concept of \"kernels\" - mathematical functions that map the data into a higher-dimensional space where it might become linearly separable. This allows SVM to find complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491f233-70b3-4d64-85c4-1e16893bbb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a223a50c-fb7d-4980-ada1-c2bbd2f12a4e",
   "metadata": {},
   "source": [
    "Certainly, let's break down the concepts of models, parameters, hyperparameters, and tuning in the context of machine learning:\n",
    "\n",
    "1. Model:\n",
    "\n",
    "Definition: A model in machine learning is a mathematical representation of a real-world phenomenon. It's like a formula or set of rules that the computer uses to learn patterns from data and make predictions.\n",
    "Examples:\n",
    "Linear Regression: A model that predicts a continuous value based on a linear relationship with input features.   \n",
    "Decision Tree: A model that creates a tree-like structure of decisions to classify data.   \n",
    "Neural Network: A complex model inspired by the human brain, with interconnected layers of nodes.   \n",
    "2. Parameters:\n",
    "\n",
    "Definition: Parameters are the internal variables that a model learns during the training process. They are adjusted to best fit the training data.   \n",
    "Examples:\n",
    "Weights and biases in a neural network: These determine the strength of connections between neurons.   \n",
    "Coefficients in a linear regression model: These determine the slope and intercept of the line.   \n",
    "3. Hyperparameters:\n",
    "\n",
    "Definition: Hyperparameters are settings that you, the machine learning engineer, choose before training the model. They control the learning process itself.   \n",
    "Examples:\n",
    "Learning rate: How much the model adjusts its parameters in each training step.   \n",
    "Number of layers and neurons in a neural network: The architecture of the model.\n",
    "The value of k in k-Nearest Neighbors: The number of neighbors to consider for classification.   \n",
    "4. Tuning:\n",
    "\n",
    "Definition: Hyperparameter tuning is the process of finding the best combination of hyperparameter values to achieve the best performance on your model.   \n",
    "Methods:\n",
    "Grid Search: Trying all possible combinations of hyperparameter values within a specified range.   \n",
    "Random Search: Randomly sampling hyperparameter values from a given distribution.   \n",
    "Bayesian Optimization: Uses a probabilistic model to intelligently explore the hyperparameter space.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8bf07b-08a8-496b-be81-4f8e3ba55cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05a811eb-76ab-4509-bc47-11e74b76b96f",
   "metadata": {},
   "source": [
    "The text you provided discusses Hyperparameter Tuning in machine learning. Here's a breakdown of the key concepts:\n",
    "\n",
    "Hyperparameters vs. Parameters\n",
    "\n",
    "Parameters: These are the internal variables that a model learns during the training process. For example, in a neural network, the weights and biases between neurons are parameters.\n",
    "Hyperparameters: These are settings that you, the machine learning engineer, control before training the model. They influence how the model learns. Examples include:\n",
    "Learning rate: How much the model adjusts its parameters in each training step.\n",
    "Number of layers and neurons in a neural network: The architecture of the model.\n",
    "The value of k in k-Nearest Neighbors: The number of neighbors to consider for classification.\n",
    "Hyperparameter Tuning\n",
    "\n",
    "The Search Problem: Finding the best combination of hyperparameter values to achieve the best performance on your model can be treated as a search problem.\n",
    "\n",
    "Two Best Strategies:\n",
    "\n",
    "GridSearchCV: This method systematically tries all possible combinations of hyperparameter values within a specified range. It's thorough but can be computationally expensive for models with many hyperparameters.\n",
    "\n",
    "RandomizedSearchCV: This method randomly samples hyperparameter values from a given distribution. It's often more efficient than GridSearchCV, especially when dealing with a large number of hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
