{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2e947c-2eb2-4078-814c-4b1084202121",
   "metadata": {},
   "source": [
    "### Regression Analysis: Linear and Nonlinear Algorithms\n",
    "\n",
    "### Regression analysis is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It aims to predict the value of the dependent variable based on the values of the independent variables.   \n",
    "\n",
    "### Linear Regression\n",
    "### Assumption: Assumes a linear relationship between the dependent and independent variables.\n",
    "### Model: y = b0 + b1x1 + b2x2 + ... + bn*xn\n",
    "### Algorithm:\n",
    "### 1. Simple Linear Regression: Models the relationship between a single independent variable and the dependent variable.\n",
    "### 2. Multiple Linear Regression: Models the relationship between multiple independent variables and the dependent variable.   \n",
    "###      Example: Predicting house prices based on size and location.\n",
    "### 3,4. Lasso and Ridge Regression are regularization techniques for linear regression that prevent overfitting by adding a penalty term to the loss function\n",
    "### with Lasso shrinking some coefficients to zero for feature selection and Ridge shrinking all coefficients without setting any to zero.\n",
    "\n",
    "### Nonlinear Regression\n",
    "### Assumption: Does not assume a linear relationship between the dependent and independent variables.\n",
    "### Model: Various models, such as polynomial, exponential, logarithmic, etc.\n",
    "### Algorithms:\n",
    "### Polynomial Regression: Models the relationship using polynomial functions.\n",
    "### Exponential Regression: Models exponential growth or decay.\n",
    "### Logarithmic Regression: Models logarithmic relationships.\n",
    "### Support Vector Regression (SVR): A powerful machine learning algorithm that can handle both linear and nonlinear regression.\n",
    "### Example: Modeling the growth of a population over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e1ca35-fe1f-4d70-a973-0a99c9c3455b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7730222-90cc-4d53-a8d0-ed92d2460aec",
   "metadata": {},
   "source": [
    "# Simple Linear Regression: The Straight Line Story\n",
    "\n",
    "### Imagine you have two things that seem to be related, like how much you study and your test score. Simple linear regression is \n",
    "### like drawing a straight line through the data points to show this relationship. It helps us understand how one thing (like study time) affects\n",
    "### another (like test score) in a simple, linear way.\n",
    "\n",
    "\n",
    "### y = mx+c\n",
    "### y = dependent variable\n",
    "### x=independent variable\n",
    "### m= slope/gradient\n",
    "### c=intercept\n",
    "\n",
    "### When to Use it:\n",
    "\n",
    "### Clear Relationship: When you think there's a straight-line connection between two things.\n",
    "### Prediction: If you want to predict one thing based on the other.\n",
    "### Understanding the Relationship: To see how much one thing changes when the other changes.\n",
    "### Example:\n",
    "\n",
    "### Let's say you want to predict someone's height based on their shoe size. You could use simple linear regression to find the best-fitting line through the data points of people's shoe sizes and heights. This line would help you estimate someone's height based on their shoe size.\n",
    "##\n",
    "# Key Points:\n",
    "\n",
    "### Simple: It deals with only two variables.\n",
    "### Linear: Assumes a straight-line relationship.\n",
    "### Predictive: Helps you make predictions based on the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964e2a4-038f-4a5e-87d6-42f05939b93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1d17512-a9c1-4da0-8326-9d9d30dc9f6a",
   "metadata": {},
   "source": [
    "# What is Multiple Linear Regression?\n",
    "\n",
    "### Imagine you want to predict the price of a house. You know that several factors can influence the price, such as the size of the house, \n",
    "### the number of bedrooms, the location, and the age of the house. Multiple linear regression is a statistical method that helps you find a \n",
    "### relationship between these factors (independent variables) and the price of the house (dependent variable).\n",
    "\n",
    "### Algorithm in Simple Words\n",
    "\n",
    "### The algorithm tries to find the best-fitting line (or plane, in higher dimensions) that represents the relationship between the independent \n",
    "### variables and the dependent variable. It does this by minimizing the difference between the actual values and the predicted values.\n",
    "\n",
    "### Formula\n",
    "\n",
    "### The formula for multiple linear regression is:\n",
    "\n",
    "### y = b0 + b1*x1 + b2*x2 + ... + bn*xn\n",
    "### where:\n",
    "\n",
    "### y is the dependent variable (e.g., house price)\n",
    "### b0 is the intercept (the value of y when all independent variables are 0)\n",
    "### b1, b2, ..., bn are the coefficients (weights) for each independent variable (x1, x2, ..., xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e874b2-f1ae-4596-b4ce-349e600f099e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8895825f-49ca-4b5e-a138-d58488597f98",
   "metadata": {},
   "source": [
    "### Imagine you're trying to predict how much a car will cost based on its age. A simple linear regression would assume the price decreases\n",
    "### at a constant rate as the car gets older, like a straight line.\n",
    "\n",
    "### But we know that's not always true! Sometimes the price drops faster in the first few years, then slows down. That's where polynomial regression\n",
    "### comes in.\n",
    "\n",
    "### Polynomial regression allows us to fit a curved line to the data instead of a straight one. This curve can better capture the non-linear\n",
    "### relationship between the car's age and its price.\n",
    "\n",
    "### In simple words, it's like using a flexible ruler instead of a rigid one to fit the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662b063-5cbd-40f9-b427-4dde3cb58ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6c636cf-7d4f-42ee-aaf5-d8760bb6a648",
   "metadata": {},
   "source": [
    "# Cost Function: The Machine Learning Scorecard\n",
    "\n",
    "### In simple terms, a cost function is like a scorecard for your machine learning model. It measures how well your model is \n",
    "### performing by calculating the difference between its predictions and the actual values.\n",
    "\n",
    "### Why is it important?\n",
    "\n",
    "### Think of it like this: you're teaching a child to throw a ball at a target. The cost function is like the distance between \n",
    "### where the ball lands and the bullseye. The smaller the distance, the better the throw (or the better your model's predictions).\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "### Prediction: Your model makes a prediction.\n",
    "### Comparison: The cost function compares this prediction to the actual value.\n",
    "### Scoring: It calculates a score based on the difference. The bigger the difference, the higher the score.\n",
    "### The goal:\n",
    "\n",
    "### The goal is to minimize this score, which means your model's predictions are getting closer and closer to the actual values. This is how machine learning models learn and improve over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0297c-a58b-4953-924e-4c677c4ebadf",
   "metadata": {},
   "source": [
    "### Loss Function: Measures the error for a single data point.\n",
    "### Cost Function: Measures the average error across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51863b-3c63-4caa-a4cc-2265bae32c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b2c9bb3-aa96-4e8b-8e7c-dd700a198e85",
   "metadata": {},
   "source": [
    "# Regression Cost Functions\n",
    "\n",
    "### Think of predicting a house price. You want your model to be as close to the actual price as possible.\n",
    "\n",
    "### Mean Squared Error (MSE):  Like calculating the average of the squared differences between your model's guess and the actual price. \n",
    "### It punishes large errors more heavily.\n",
    "\n",
    "###  Mean Absolute Error (MAE): Similar to MSE, but calculates the average of the absolute differences. Less sensitive to outliers than MSE.\n",
    "\n",
    "# Classification Cost Functions\n",
    "\n",
    "### Imagine predicting whether an email is spam or not. You want your model to be confident in its decision.\n",
    "\n",
    "### Binary Cross-Entropy: Used for two-class problems (spam/not spam). Measures how well your model predicts the probability of each class.\n",
    "\n",
    "### Categorical Cross-Entropy: Used for multiple-class problems (e.g., classifying images into different types of animals). Measures how well \n",
    "### your model predicts the probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438812c1-ecb4-4bb1-8f8a-1f7ca7682b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b53bd24-b45a-4f3a-a650-c502f4643c40",
   "metadata": {},
   "source": [
    "## Mean Squared Error\n",
    "\n",
    "### Mean Squared Error (MSE) is a common metric used to evaluate the performance of a regression model. It quantifies the average squared difference between the actual values (y) and the predicted values (ŷ).\n",
    "\n",
    "### Formula:\n",
    "\n",
    "### MSE = (1/n) * Σ(yi - ŷi)^2\n",
    "### where:\n",
    "\n",
    "### n is the number of data points\n",
    "### yi is the actual value\n",
    "### ŷi is the predicted value\n",
    "### Interpretation:\n",
    "\n",
    "### A lower MSE indicates that the model's predictions are closer to the actual values, suggesting better performance.\n",
    "### A higher MSE implies that the model's predictions are further away from the actual values, indicating poorer performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a089204-e542-4fcc-b31e-f9bba4535acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46bb92f7-c61f-4dc8-a80c-f6b2dd3e5ba0",
   "metadata": {},
   "source": [
    "# Root Mean Squared Error (RMSE)\n",
    "\n",
    "### Definition:\n",
    "\n",
    "### RMSE is the square root of the mean squared difference between the actual values and the predicted values. It's essentially the square root of MSE.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "### RMSE = √(1/n) * Σ(yi - ŷi)^2\n",
    "### where:\n",
    "\n",
    "### n is the number of data points\n",
    "### yi is the actual value\n",
    "### ŷi is the predicted value\n",
    "### Interpretation:\n",
    "\n",
    "### RMSE provides a measure of the average magnitude of the errors in the same units as the target variable.\n",
    "### A lower RMSE indicates better model performance, as the average error is smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f1961-9373-4a3d-a26e-19d7c3a206ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71ae9059-04dc-4d94-833e-6395e4a12182",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "### It is a technique used in machine learning to prevent overfitting.\n",
    "### Overfitting happens when a model performs very well on the training data but poorly on new, unseen data. \n",
    "### This is because the model has learned the training data too well, including its noise and quirks, and can't generalize to new situations.\n",
    "\n",
    "### Imagine you're teaching a child to recognize different animals. You show them pictures of dogs, cats, and birds.\n",
    "### If you only show them pictures of one breed of dog, they might learn to recognize that specific breed but not other dog breeds. \n",
    "### This is similar to overfitting.\n",
    "\n",
    "### Regularization techniques help prevent this by adding a penalty term to the model's learning process. This penalty discourages \n",
    "### the model from becoming too complex and relying too heavily on any single feature.\n",
    "\n",
    "### There are two main types of regularization:\n",
    "\n",
    "### L1 Regularization (Lasso): This technique adds a penalty to the model's coefficients (the numbers that determine how much each feature \n",
    "### contributes to the prediction). This penalty encourages some coefficients to become exactly zero, effectively removing those features from the model.\n",
    "### This can be useful for feature selection, as it helps identify the most important features.\n",
    "\n",
    "### L2 Regularization (Ridge): This technique also adds a penalty to the model's coefficients, but it doesn't force them to be exactly zero. \n",
    "### Instead, it encourages them to be small. This helps to prevent any single feature from having too much influence on the model's predictions.\n",
    "\n",
    "### Regularization is a powerful tool that can help improve the performance of machine learning models, especially when dealing with complex datasets \n",
    "### or when there is a risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90ed5d-32ef-46ab-9790-95644001ddee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2700b08-3ee2-42a4-9df4-f4f8ab5c183a",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "\n",
    "### This is a regularization technique used in feature selection using a Shrinkage method also referred to as the penalized regression1 method.   \n",
    "\n",
    "### Lasso Regression magnitude of coefficients can be exactly zero.\n",
    "### Cost function = Loss + λ Σ||w||\n",
    "\n",
    "### Loss = sum of squared residual\n",
    "### λ = penalty\n",
    "### w = slope of the curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20dd3e-7ded-40ec-a6a5-20991794e029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26dd65a7-e8ce-4825-be04-5715c0855f62",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "### Ridge Regression, also known as L2 regularization, is an extension to linear Regression that introduces a regularization term to reduce model complexity and help prevent overfitting. 1    \n",
    "\n",
    "### Ridge Regression is working value/magnitude of coefficients is almost equal to zero.\n",
    "\n",
    "### Cost function = loss + λ 2 ||w||2\n",
    "\n",
    "### Loss = sum of squared residual\n",
    "### λ = penalty\n",
    "### W = slope of the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655ebc8-f4f5-4e1a-9afd-58c96e480e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a76c3b-2c1f-4b0f-9267-9fbe1fcc99d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8538cdb7-2bac-4d41-be25-b7129d687c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24d554-83e2-49d7-81ac-885cea852f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
