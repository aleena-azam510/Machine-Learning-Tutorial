{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638fd83d-884f-4365-920c-7ac3b314c583",
   "metadata": {},
   "source": [
    "Classification\n",
    "\n",
    "The Classification algorithm is used to identify the category of new observations on the basis of training data.\n",
    "\n",
    "In Classification, a program learns from the given dataset or observations and then classifies new observation into a number of classes or groups.   \n",
    "\n",
    "Such as, Yes or No, 0 or 1, Spam or Not Spam, cat or dog, etc. Classes can be called as targets/labels or categories.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2420ba1-2014-4fa9-84ce-8519fbbbee09",
   "metadata": {},
   "source": [
    "There are two types of Classifications:\n",
    "\n",
    "Binary Classifier: If the classification problem has only two possible outcomes, then it is called as Binary Classifier.\n",
    "\n",
    "Examples: SPAM or NOT SPAM, CAT or DOG, etc.\n",
    "Multi-class Classifier: If a classification problem has more than two outcomes, then it is called as Multi-class Classifier.\n",
    "\n",
    "Example: Classifications of types of crops, Classification of types of music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da7565-42fd-4855-a648-c109df2c35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES OF ML\n",
    "\n",
    "CLASSIFICATION ALGORITHMS\n",
    "\n",
    "Non-linear Models\n",
    "\n",
    "K-Nearest Neighbours\n",
    "SVM\n",
    "Naïve Bayes\n",
    "Decision Tree Classification\n",
    "Random Forest Classification\n",
    "\n",
    "Linear Models\n",
    "\n",
    "Logistic Regression\n",
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef26179-c345-42b5-bae1-fb63a484f1c9",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION\n",
    "\n",
    "Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique.\n",
    "\n",
    "It is used for predicting the categorical dependent variable using a given set of independent variables.   \n",
    "\n",
    "Therefore, the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.\n",
    "\n",
    "\n",
    "Types of Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical method used for predicting the probability of an event occurring. It's particularly useful when the dependent variable is categorical, meaning it can take on a limited number of values.\n",
    "\n",
    "The image categorizes Logistic Regression into three main types based on the nature of the dependent variable:\n",
    "\n",
    "Binomial Logistic Regression:\n",
    "\n",
    "This is the most common type.\n",
    "The dependent variable has only two possible outcomes (binary).\n",
    "Examples:\n",
    "Pass/Fail\n",
    "Yes/No\n",
    "0/1\n",
    "Multinomial Logistic Regression:\n",
    "\n",
    "The dependent variable can have three or more unordered categories.\n",
    "Examples:\n",
    "Types of animals (cat, dog, sheep)\n",
    "Colors (red, green, blue)\n",
    "Ordinal Logistic Regression:\n",
    "\n",
    "The dependent variable has three or more ordered categories.\n",
    "The order of the categories is meaningful.\n",
    "Examples:\n",
    "Low, Medium, High\n",
    "Strongly disagree, Disagree, Neutral, Agree, Strongly agree\n",
    "In essence:\n",
    "\n",
    "Binomial: Two choices\n",
    "Multinomial: Multiple unordered choices\n",
    "Ordinal: Multiple ordered choices\n",
    "\n",
    "\n",
    "\n",
    "Equation\n",
    "\n",
    "The core equation for Logistic Regression is:\n",
    "\n",
    "y = 1 / (1 + e^(-x))\n",
    "y: This represents the probability of the event occurring (dependent variable). In the example given, it's the probability of someone buying a product.\n",
    "x: This represents the input variable (independent variable). In the example, it's the person's salary.\n",
    "e: This is Euler's constant, approximately equal to 2.71828.\n",
    "How it Works\n",
    "\n",
    "Linear Regression Foundation: The Logistic Regression equation is derived from the Linear Regression equation. However, unlike Linear Regression, which predicts a continuous value, Logistic Regression predicts a probability.\n",
    "\n",
    "Sigmoid Function: The equation uses the sigmoid function (also known as the logistic function), which maps any real value to a value between 0 and 1. This makes it suitable for predicting probabilities.\n",
    "\n",
    "Probability Prediction: By plugging the input value (x) into the equation, you get a value between 0 and 1. This value represents the predicted probability of the event occurring.\n",
    "\n",
    "Example\n",
    "\n",
    "Let's say the equation is:\n",
    "\n",
    "y = 1 / (1 + e^(-0.01 * salary))\n",
    "If someone's salary is 50,000:\n",
    "\n",
    "y = 1 / (1 + e^(-0.01 * 50,000))\n",
    "Calculating this gives you a probability value between 0 and 1, representing the likelihood of that person buying the product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fffa6-7711-4919-81b0-27d20ba8628e",
   "metadata": {},
   "source": [
    "workflow of logistic\n",
    "data > input > linear regression>output>sigmoid function>true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b60aa0-21a5-457b-9fd0-83370826f8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30004595-276a-4992-8898-a3611f5f8ba2",
   "metadata": {},
   "source": [
    "A confusion matrix is like a report card for this model.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "The Model Predicts: The model looks at each email and guesses if it's spam or not.\n",
    "Reality Check: We compare the model's guesses to the actual truth (whether the email really was spam or not).\n",
    "The Matrix: The confusion matrix shows us how often the model got things right and wrong.\n",
    "Here's a simple example:\n",
    "\n",
    "Prediction\tActual Spam\tActual Not Spam\n",
    "Spam\t    80\t10\n",
    "Not Spam\t5\t95\n",
    "\n",
    "Export to Sheets\n",
    "Top Left (80): The model correctly identified 80 spam emails as spam (True Positives).\n",
    "Top Right (10): The model incorrectly flagged 10 non-spam emails as spam (False Positives).\n",
    "Bottom Left (5): The model missed 5 spam emails (False Negatives).\n",
    "Bottom Right (95): The model correctly identified 95 non-spam emails as non-spam (True Negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a1b6ef-7311-4f7c-b093-a1715e5a7b7d",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "Accuracy measures how often a model is correct overall. It's the proportion of correct predictions (true positives and true negatives) out of all predictions made.\n",
    "\n",
    "Precision\n",
    "Precision measures how often a model is correct when it predicts \"yes\" or \"positive\". It's the proportion of true positives out of all positive predictions.\n",
    "\n",
    "Recall\n",
    "Recall measures how often a model detects all the actual \"yes\" or \"positive\" cases. It's the proportion of true positives out of all actual positive cases.\n",
    "\n",
    "F1 Score\n",
    "The F1 score is the average of precision and recall. It balances the two and provides a single metric to evaluate the model's performance.\n",
    "\n",
    "Confusion Matrix\n",
    "A confusion matrix is a table that helps evaluate a model's performance. It has four main parts:\n",
    "True Positives (TP): Correctly predicted \"yes\" or \"positive\" cases.\n",
    "True Negatives (TN): Correctly predicted \"no\" or \"negative\" cases.\n",
    "False Positives (FP): Incorrectly predicted \"yes\" or \"positive\" cases.\n",
    "False Negatives (FN): Incorrectly predicted \"no\" or \"negative\" cases.\n",
    "\n",
    "Here's an example of a confusion matrix:\n",
    "Predicted\tActual Positive\tActual Negative\n",
    "Positive\tTP (100)\tFP (20)\n",
    "Negative\tFN (15)\t    TN (150)\n",
    "\n",
    "In this example:\n",
    "Accuracy = (TP + TN) / Total = (100 + 150) / 285 ≈ 0.87\n",
    "Precision = TP / (TP + FP) = 100 / (100 + 20) ≈ 0.83\n",
    "Recall = TP / (TP + FN) = 100 / (100 + 15) ≈ 0.87\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall) ≈ 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a359ed-4807-4472-8c12-8f5eb588a049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d88f38e-63c7-4909-96f6-eee836bf34b2",
   "metadata": {},
   "source": [
    "Techniques to Handle Imbalanced Datasets\n",
    "1. Random Over Sampling (ROS)\n",
    "How it works: Randomly duplicate samples from the minority class to increase its size.\n",
    "Pros: Simple to implement, increases the size of the minority class.\n",
    "Cons: May lead to overfitting, as the model sees duplicate samples.\n",
    "2. Random Under Sampling (RUS)\n",
    "How it works: Randomly remove samples from the majority class to decrease its size.\n",
    "Pros: Reduces the size of the dataset, may improve model training time.\n",
    "Cons: May discard important information, can lead to loss of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a238081-39ff-4754-9e69-2e5e179610e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c47bd1c-0286-4969-beb0-f83e5ee49cae",
   "metadata": {},
   "source": [
    "Naive Bayes is a machine learning algorithm that helps predict the likelihood of an event occurring based on prior knowledge of conditions that might be related to the event.\n",
    "How Does it Work?\n",
    "Bayes' Theorem: Naive Bayes is based on Bayes' theorem, which describes the probability of an event occurring given some prior knowledge of conditions.\n",
    "Prior Probability: The algorithm starts with a prior probability of each class (e.g., spam or not spam).\n",
    "Likelihood: It then calculates the likelihood of each feature (e.g., words in an email) given each class.\n",
    "Posterior Probability: The algorithm multiplies the prior probability by the likelihood to get the posterior probability of each class given the features.\n",
    "Prediction: The class with the highest posterior probability is chosen as the prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
